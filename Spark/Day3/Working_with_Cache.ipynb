{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7e8b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSession\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b03d4-049d-45ef-8b6d-0bf45eeb1703",
   "metadata": {},
   "source": [
    "# Caching\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "* Understaning for How caching works?\n",
    "* Explore the various caching mechanisims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c293410a-3621-4667-b739-2482700fdba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = (\n",
    "    spark.read.option(\"header\", \"true\").csv(\"spark-data/ratings.csv\")\n",
    "    .withColumn(\"rating\", col(\"rating\").cast(\"float\"))\n",
    "    .filter(col(\"rating\") > 3)\n",
    "    .groupBy(\"rating\")\n",
    "    .count()\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "df.count()\n",
    "df.count()\n",
    "df.write.mode(\"overwrite\").csv(\"spark-data/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d7d628-0fb8-4fc0-8bf6-0a53f177868e",
   "metadata": {},
   "source": [
    "## A Fresh Start\n",
    "For this section, first of all there is need to clear the existing cache.\n",
    "\n",
    "There are several ways to accomplish this:\n",
    "  * Remove each cache one-by-one which is fairly problematic\n",
    "  * Restart the cluster - takes a fair while to come back online\n",
    "  * Just blow the entire cache away - this will affect each and every user on the cluster!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "212fd4d7-261c-4fed-87f8-8dba5d59c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! DO NOT RUN THIS ON A SHARED CLUSTER !!!\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "#!!! It will Delete the cache of your system and Your's Co-Worker's !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95842137-fa52-4270-b849-595d294f6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "ratingsDF = (spark.read\n",
    "  .option(\"header\",\"true\")\n",
    "  .csv(\"spark-data/ratings.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70db431-3cb4-4ea3-8032-34f89839540c",
   "metadata": {},
   "source": [
    "The 646Mb data is currently in HDFS, which means each time you scan through it, your Spark cluster has to read the 646 MB of data remotely over the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f72900-7533-4b69-ad73-927914ea7ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ratingsDF\n",
    " .count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27cc1fa-99aa-43a3-87e8-0591142a50de",
   "metadata": {},
   "source": [
    "The ratings DataFrame contains 25 million rows.\n",
    "\n",
    "Do Make a note of how long the previous operation takes.\n",
    "\n",
    "Re-run it several times so as trying to establish an average.\n",
    "\n",
    "Now Let's try a slightly more complicated operation, such as sorting, which induces an \"expensive\" shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d5a6cc-2bdd-425c-8970-85c986b5e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ratingsDF\n",
    " .orderBy(\"movieId\")\n",
    " .count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3369403f-1761-4e53-91b5-50ef9c305628",
   "metadata": {},
   "source": [
    "Each and Every time we re-run these operations, it goes all the way back to the original data store.\n",
    "\n",
    "This requires pulling all the data across the network for every execution.\n",
    "\n",
    "In most of the cases, this network IO is the most expensive part of a job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb00fee3-ca0e-4635-b021-568945a71562",
   "metadata": {},
   "source": [
    "## cache()\n",
    "\n",
    "We can avoid all of this overhead by caching the data on the executors.\n",
    "\n",
    "Just go ahead and run the following command.\n",
    "\n",
    "Don't forget to make a note of how long it takes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b6a692-789a-413c-9da3-eae6692709f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsDF.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9fe9dd-f03d-46e2-815c-bb68e070615a",
   "metadata": {},
   "source": [
    "The last `count()` will take a little longer than normal.\n",
    "\n",
    "It has to perform the cache and do the work of materializing the cache.\n",
    "\n",
    "Now the `pageviewsDF` is cached **AND** the cache has been materialized.\n",
    "\n",
    "Before we rerun our queries, check the **Spark UI** and the **Storage** tab.\n",
    "\n",
    "Now, run the two queries and compare their execution time to the ones above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561bb6dc-be30-4435-9184-c4ecb7c4a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720fc1c8-818e-4c58-8369-76416169e62d",
   "metadata": {},
   "source": [
    "Was it Faster?\n",
    "\n",
    "All of our data is being stored in cache on the executors.\n",
    "\n",
    "We are no longer making network calls. Our plain `count()` should be sub-second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f53a7-5bcd-46be-8954-ce9d02a78778",
   "metadata": {},
   "source": [
    "# Caching Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c81a46b1-563e-456f-8a52-027d9fb627ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "pageViewsDF = (spark.read\n",
    "  .parquet(\"spark-data/pageviews_by_second_par\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a249980f-0e49-4844-9b55-8c1a129aeb15",
   "metadata": {},
   "source": [
    "Size on disk of above parquet files is 100 MB. Let's apply the cache and see how much storage is taken by the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619573fd-b993-45aa-aadc-453bc4225f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageViewsDF.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d505455-ebf2-4d81-99c2-94e87989f917",
   "metadata": {},
   "source": [
    "##### Now go Storage Tab of Spark UI and check the size in memory\n",
    "\n",
    "##### Did you noticed size in memory is 4 times the size of file on disk?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1200015-4eb1-4449-aeea-bdf7a9cdc53e",
   "metadata": {},
   "source": [
    "It is because cache() method save the data in memory in deserialized format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3fd6ef-e604-4e8d-8b6d-28e4eb997cbe",
   "metadata": {},
   "source": [
    "## persist()\n",
    "\n",
    "`cache()` is just an alias for the `persist()`\n",
    "\n",
    "Let's take a look at the API docs for:\n",
    "\n",
    "* `Dataset.persist()` - Scala\n",
    "* `DataFrame.persist()` - Python\n",
    "\n",
    "`persist()` allows one to specify an additional parameter i.e. storage level, indicating how the data is cached:\n",
    "\n",
    "* DISK_ONLY\n",
    "* DISK_ONLY_2\n",
    "* MEMORY_AND_DISK\n",
    "* MEMORY_AND_DISK_2\n",
    "* MEMORY_AND_DISK_SER\n",
    "* MEMORY_AND_DISK_SER_2\n",
    "* MEMORY_ONLY\n",
    "* MEMORY_ONLY_2\n",
    "* MEMORY_ONLY_SER\n",
    "* MEMORY_ONLY_SER_2\n",
    "* OFF_HEAP\n",
    "\n",
    "** *Note:* ** *The default storage level for:*\n",
    "* *RDDs are **MEMORY_ONLY**.*\n",
    "* *DataFrames are **MEMORY_AND_DISK**.* \n",
    "* *Streaming is **MEMORY_AND_DISK_2**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a253c4d7-1e74-4425-a443-9b6c88c8f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "pageviewsParDF = (spark.read\n",
    "  .parquet(\"spark-data/pageviews_by_second_par\")\n",
    ")\n",
    "pageviewsParDF.persist(StorageLevel.MEMORY_AND_DISK).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feed5c6-f2a6-40d6-a85b-c09cf1c31065",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageviewsParDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a28253-9c24-428c-b48d-deec9064bede",
   "metadata": {},
   "source": [
    "Go to Spark UI Storage Tab again\n",
    "\n",
    "##### Did you noticed size in memory is 2 times the size of file on disk?\n",
    "Since Storage Level  is Memory Disk Serialized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f1d92-ce9e-4a11-ad31-c38d7fa8b34b",
   "metadata": {
    "id": "_zLRtnJYXOSs"
   },
   "source": [
    "It's bigger in memory than on disk! Why? Due to Java string object storage.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/tuning/java-string.png\" alt=\"Java String Memory allocation\"/><br/>\n",
    "\n",
    "\n",
    "- A regular 4 byte string would end up taking 48 bytes. \n",
    "- The diagram shows how the 40 bytes are allocated and we also need to round up byte usage to be divisible of 8 due to JVM padding. \n",
    "- This is a very bloated representation knowing that of these 48 bytes, we're actually after only 4. \n",
    "\n",
    "Let's try with `inferSchema` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca07c083-442c-4eb5-98a5-9fab6a6e4404",
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF = pageviewsParDF.withColumn(\n",
    "    \"timestamp_long\",\n",
    "    to_timestamp(\"timestamp\", \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"long\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f7694b-1bd3-436d-a0e4-15c4537ea191",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageviewsParDF.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4654239-1f60-40c5-9487-233a6b7d0c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46daff32-9e4c-4c97-868a-f12a87fe6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF.persist(StorageLevel.DISK_ONLY).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338bd2f3-8006-42ea-8ca3-543937668904",
   "metadata": {},
   "source": [
    "#### Did you noticed that just by changing the type from string to long, there is a difference of almost 50 MB?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8451a9-3803-4370-ac36-30a0ad440c69",
   "metadata": {
    "id": "ZLQB9-U2XOSo"
   },
   "source": [
    "## RDD Name\n",
    "\n",
    "If you haven't noticed yet, the **RDD Name** on the **Storage** tab in the **Spark UI** is a big ugly name.\n",
    "\n",
    "It's a bit hacky, but there is a workaround for assigning a name.\n",
    "0. Create your `DataFrame`.\n",
    "0. From that `DataFrame`, create a temporary view with any name.\n",
    "0. Specifically, cache the table via the `SparkSession` and its `Catalog`.\n",
    "0. Materialize the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d392a-41a3-4b5d-b4dd-f0f03185bd23",
   "metadata": {
    "id": "otGvy5e9XOSp",
    "outputId": "79feb4a2-d639-4881-93f3-7085024ad4b4"
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark import StorageLevel\n",
    "pageviewsParDF = (spark.read\n",
    "  .parquet(\"spark-data/pageviews_by_second_par\")\n",
    ")\n",
    "pageviewsParDF.createOrReplaceTempView(\"Pageviews_DF_Python\")\n",
    "spark.catalog.cacheTable(\"Pageviews_DF_Python\")\n",
    "\n",
    "pageviewsParDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef0b11-8b02-4882-a20d-7273c52f14e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageviewsParDF.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127cbf04-60ca-41a8-aea7-ebb734869e50",
   "metadata": {},
   "source": [
    "Using OffHeap Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e2d7e-0df2-4d48-a349-2df0ffec0c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "pageviewsParDF = (spark.read\n",
    "  .parquet(\"spark-data/pageviews_by_second_par\")\n",
    ")\n",
    "pageviewsParDF.persist(StorageLevel.OFF_HEAP).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace21f3d-3457-4125-873f-8165ae801bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageviewsParDF.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f06ae7-6491-4c25-a614-b74817f3a45a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
