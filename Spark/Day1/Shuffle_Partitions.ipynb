{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wpOGuG9jDb8u",
   "metadata": {
    "id": "wpOGuG9jDb8u"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ColabSparkSession\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba5b91-9a14-4add-a766-5c9a231c80e6",
   "metadata": {
    "id": "87ba5b91-9a14-4add-a766-5c9a231c80e6"
   },
   "source": [
    "<font color=\"orange\"><b> Shuffle Partitions </b> </font>\n",
    "\n",
    "__Wide Transformations__ requires data shuffling over network thus contributing to multiple output partitions.\n",
    "\n",
    "\n",
    "Before Spark 3.0, no of shuffle partitions depends on spark property\n",
    "- spark.sql.shuffle.partitions (default- 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6143e55-09dd-4253-9bb1-cb4f4deb8ac4",
   "metadata": {
    "id": "b6143e55-09dd-4253-9bb1-cb4f4deb8ac4"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "spark.read.option(\"header\", \"true\").csv(\"spark-data/ratings.csv\") \\\n",
    "    .withColumn(\"rating\", col(\"rating\").cast(\"float\")) \\\n",
    "    .filter(col(\"rating\") > 3) \\\n",
    "    .groupBy(\"rating\") \\\n",
    "    .count() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fde1c5d-f166-4e0f-8fb6-58c3a9a9d6c4",
   "metadata": {
    "id": "3fde1c5d-f166-4e0f-8fb6-58c3a9a9d6c4"
   },
   "source": [
    "___Did you noticed there are two jobs because of above code.\n",
    "Second Jobs have 200 tasks___\n",
    "\n",
    "- <font color=\"sky blue\">Remember Number of Task = Number of Partitions</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df301ec4-9ac6-4630-a855-f05a60e55200",
   "metadata": {
    "id": "df301ec4-9ac6-4630-a855-f05a60e55200"
   },
   "source": [
    "Let's change sql shuffle partitions property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bde178f-342f-496c-9496-3788b3d93c41",
   "metadata": {
    "id": "8bde178f-342f-496c-9496-3788b3d93c41"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dbbaab-68de-4c03-b64e-ddf87fbeb7dc",
   "metadata": {
    "id": "83dbbaab-68de-4c03-b64e-ddf87fbeb7dc"
   },
   "source": [
    "#### Run below code and check Spark UI again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9c603-6f35-4999-994c-6784c8689da8",
   "metadata": {
    "id": "71d9c603-6f35-4999-994c-6784c8689da8"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "spark.read.option(\"header\", \"true\").csv(\"spark-data/ratings.csv\") \\\n",
    "    .withColumn(\"rating\", col(\"rating\").cast(\"float\")) \\\n",
    "    .filter(col(\"rating\") > 3) \\\n",
    "    .groupBy(\"rating\") \\\n",
    "    .count() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432647f1-e5bb-4297-bc58-82b1b9277026",
   "metadata": {
    "id": "432647f1-e5bb-4297-bc58-82b1b9277026"
   },
   "source": [
    "___Did you noticed there are two jobs because of above code.\n",
    "Second Jobs have only 8  tasks___\n",
    "\n",
    "This verifies two things:\n",
    "- Number of shuffle partitions is decided by spark shuffle property\n",
    "- Number of task = Number of partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0d322a-038a-4508-874a-e0315792530a",
   "metadata": {
    "id": "cf0d322a-038a-4508-874a-e0315792530a"
   },
   "source": [
    "### <font color=\"orange\">Assignment</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d926ff-775c-47fd-9220-f95803177ac6",
   "metadata": {
    "id": "27d926ff-775c-47fd-9220-f95803177ac6"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"false\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",200)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "peopleDF = spark.read.option(\"header\",\"true\").csv(\"spark-data/people-10m\")\n",
    "peopleDF \\\n",
    ".withColumn(\"birthyear\",year(col(\"birthDate\"))) \\\n",
    ".groupBy(\"birthyear\") \\\n",
    ".count() \\\n",
    ".write.mode(\"overwrite\").parquet(\"spark-data/people-10m/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5261ac10-0ab9-4551-9dd0-b3bacccb35d0",
   "metadata": {
    "id": "5261ac10-0ab9-4551-9dd0-b3bacccb35d0"
   },
   "source": [
    "### Change Shuffle Partitions\n",
    "\n",
    "Decide how many partitions are best suited for this job after shuffle stage?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
