{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bca1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ColabSparkSession\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b3ab5d-a232-40f6-a4e9-da63336405ee",
   "metadata": {
    "id": "68b3ab5d-a232-40f6-a4e9-da63336405ee",
    "tags": []
   },
   "source": [
    "<font color=\"#208FAC\">__Understanding Execution Plan__ </font>\n",
    "\n",
    "Make sure to disable Spark Adaptive Query Plan\n",
    "Make sure to keep default number of shufflle partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a025f339-0d48-42cf-a878-6296c87b331d",
   "metadata": {
    "executionInfo": {
     "elapsed": 1501,
     "status": "ok",
     "timestamp": 1752649376069,
     "user": {
      "displayName": "Shivam",
      "userId": "12658021398747847138"
     },
     "user_tz": -330
    },
    "id": "a025f339-0d48-42cf-a878-6296c87b331d"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"false\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83666e0-36f3-488b-bcaf-988a234131a3",
   "metadata": {
    "id": "b83666e0-36f3-488b-bcaf-988a234131a3"
   },
   "source": [
    "## Write below code\n",
    "\n",
    "##### Step1: Read tags.csv from hdfs at location \"spark-data/tags.csv\"\n",
    "##### While reading, remember to set header - true\n",
    "##### Step4: Group movies based on movieId\n",
    "##### Step5: count them\n",
    "##### Step6: write output back to hdfs in csv format at location \"spark-data/output/tags\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f21e311-7700-437a-b5ec-fe4ce8b26fa0",
   "metadata": {
    "executionInfo": {
     "elapsed": 39662,
     "status": "ok",
     "timestamp": 1752649415733,
     "user": {
      "displayName": "Shivam",
      "userId": "12658021398747847138"
     },
     "user_tz": -330
    },
    "id": "5f21e311-7700-437a-b5ec-fe4ce8b26fa0"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"false\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",200)\n",
    "\n",
    "spark.read.option(\"header\",\"true\") \\\n",
    ".csv(\"spark-data/tags.csv\") \\\n",
    ".groupBy(\"movieId\") \\\n",
    ".count() \\\n",
    ".write.mode(\"overwrite\") \\\n",
    "    .csv(\"spark-data/output/tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dcbc16-5ab6-422d-ad4a-203f302308b2",
   "metadata": {
    "id": "a1dcbc16-5ab6-422d-ad4a-203f302308b2"
   },
   "source": [
    "#### <font color=\"orange\"> Now check SparkUI and find out </font>\n",
    "\n",
    "1. Give me the name of all wide transformation\n",
    "2. Check SQL tab in Spark UI\n",
    "3. How many jobs were trigerred?\n",
    "4. How many stages are created?\n",
    "5. Does number of stages depends on number of wide transformation?\n",
    "6. How many tasks are there?\n",
    "7. Decide good number of spark.sql.shuffle.partitions  and run the same program again\n",
    "#### <font color=\"#52A796\">spark.conf.set(\"spark.sql.shuffle.partitions\",\"your_decided_number\")</font>\n",
    "8. Did you noticed any difference in the duration of jobs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f4d77b-c455-4504-9d40-88ea2dd014ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "error",
     "timestamp": 1752649415742,
     "user": {
      "displayName": "Shivam",
      "userId": "12658021398747847138"
     },
     "user_tz": -330
    },
    "id": "a1f4d77b-c455-4504-9d40-88ea2dd014ef",
    "outputId": "e7859a32-7408-4c50-9dfd-d5546e3985ee"
   },
   "outputs": [],
   "source": [
    "1. groupBy\n",
    "2. Done\n",
    "3. 2 jobs were triggered\n",
    "4. 3 total stages created, 1st job - 2 and 2nd job - 1\n",
    "5. Yes the number of stages depends on number of wide transformation\n",
    "6. Number of tasks, 2nd job - 202, 1stjob - 1\n",
    "7. 1\n",
    "8. Yes\n",
    "\n",
    "Difference after enabling adaptive query - 54 ms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cff6f5-a9c5-4805-bf02-058fce473b83",
   "metadata": {
    "id": "88cff6f5-a9c5-4805-bf02-058fce473b83"
   },
   "source": [
    "<font color=\"orange\"> Now Enable the Adaptive Query Engine </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5b9443c-99b8-4751-945d-0a7f5930958d",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1752649423358,
     "user": {
      "displayName": "Shivam",
      "userId": "12658021398747847138"
     },
     "user_tz": -330
    },
    "id": "f5b9443c-99b8-4751-945d-0a7f5930958d"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6aa496-0715-46e9-ae73-9fe2c1f299cc",
   "metadata": {
    "id": "bf6aa496-0715-46e9-ae73-9fe2c1f299cc"
   },
   "source": [
    "Run your code again. Check Spark UI and find out how many tasks AQE decided for this code?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7c3bb0-a62a-4fd8-b165-c5c96adaf0aa",
   "metadata": {
    "id": "3f7c3bb0-a62a-4fd8-b165-c5c96adaf0aa"
   },
   "source": [
    "What is the difference between the durations of jobs?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
