{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ColabSparkSession\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "(spark\n",
    " .read\n",
    " .option(\"header\",\"true\")\n",
    " .csv(\"spark-data/ratings.csv\")\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .partitionBy(\"rating\")\n",
    " .saveAsTable(\"rating_partioned\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"Describe formatted rating_partioned\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark\n",
    " .read\n",
    " .table(\"rating_partioned\")\n",
    " .filter(\"rating=3.5\")\n",
    " .write.mode(\"overwrite\").csv(\"spark-data/output\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark\n",
    " .read\n",
    " .option(\"header\",\"true\")\n",
    " .csv(\"spark-data/ratings.csv\")\n",
    " .filter(\"rating=3.5\")\n",
    " .write.mode(\"overwrite\").csv(\"spark-data/output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark\n",
    " .read\n",
    " .table(\"rating_partioned\")\n",
    " .filter(col(\"rating\").between(3.0,4.0))\n",
    " .explain(\"Formatted\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose a partitioning column\n",
    "2. Partition the data \n",
    "3. Read the partioned datasets, apply some filter on partitioned column, check out plans\n",
    "Do you see anything under Partitioned Filter?\n",
    "4. Write the filtered dataset , check you Spark UI SQL tab, did you see data pruning at file scan level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"spark-data/pageviews_by_second_par\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of 04-Partitioning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "name": "Partitioning",
  "notebookId": 9689
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
