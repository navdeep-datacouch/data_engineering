{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4755892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ColabSparkSession\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a56e10-e7d8-4bd1-bbea-10e2ba8f47b4",
   "metadata": {},
   "source": [
    "# <font color=\"orange\">Understanding Spark partition </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebaf571-a5c4-4a6e-ab18-cee98b54564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Run below command to get block information about the file\n",
    "\n",
    "hdfs fsck /user/training/spark-data/rating-small/ -blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b7d3e7-d94d-47d9-bfd9-7cb3a3ef7691",
   "metadata": {},
   "source": [
    "### Create a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cfc8b2-b9db-4018-b790-c710b3b410ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "smallratingDF = spark.read.parquet(\"spark-data/rating-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e112ed43-da3f-49fa-be27-2ec64bf3db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "smallratingDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b2b3e-285e-467d-b269-9cb330d40303",
   "metadata": {},
   "source": [
    "__Spark decided to read these small 601 files in 22 partition__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ee2671-f341-482d-906d-55d8de0f6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hugeratingDF = spark.read.parquet(\"spark-data/rating-huge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875942d0-fe83-4567-a5aa-674168e9b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "hugeratingDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2edc957-b59a-48ad-a26d-92537bb6aef9",
   "metadata": {},
   "source": [
    "__Spark decided to read these small 1 files in 2 partition__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce55be0-f377-4e3a-8627-9a92b9dbcc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingDF = spark.read.option(\"header\",\"true\").csv(\"spark-data/ratings.csv\")\n",
    "ratingDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0bb7539-648c-40d6-87e4-60cce675d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",200)\n",
    "ratingDF.groupBy(\"rating\").count().write.mode(\"overwrite\").csv(\"spark-data/output/ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425472be-eab6-48ec-a535-a44dab2fe1e5",
   "metadata": {},
   "source": [
    "__This time spark decided to read this 646MB file in 6 partitions__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6351cc-7cf6-4d9d-9f12-999e97168cf8",
   "metadata": {},
   "source": [
    "<font color=\"sky blue\">Question: How Spark deciding on number of partition? Is it random or based on some formula?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c93de-e595-4079-beee-e63f9c03bf0f",
   "metadata": {},
   "source": [
    " Calculation of Spark Partition depends on below properties:\n",
    "        \n",
    "- spark.default.parallelism (default: Total No. of CPU cores)\n",
    "- spark.sql.files.maxPartitionBytes (default: 128 MB)\n",
    "- spark.sql.files.openCostInBytes (default: 4 MB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34cc2cb-90e6-456e-867a-ec443ef2148c",
   "metadata": {},
   "source": [
    "First it calculates bytePercore which is based on \n",
    "\n",
    "- sum of size of all files\n",
    "- no of files\n",
    "- total no of cores of executors (default.parallelism)\n",
    "- spark.sql.files.openCostInBytes (default: 4 MB)\n",
    "\n",
    "<font color=\"light blue\">___bytesPerCore = (Sum of sizes of all data files + No. of files * openCostInBytes) / default.parallelism___</font>\n",
    "\n",
    "\n",
    "Then we compare above calculated value with spark.sql.files.maxPartitionBytes (default: 128 MB),\n",
    "whichever is minimum that becomes our partition size\n",
    "\n",
    "<font color=\"light blue\">maxSplitBytes = Minimum(maxPartitionBytes, bytesPerCore)</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8abcf29-598a-4382-a3ac-f348ccdd13f8",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> Assignment </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb91f695-facd-4f7b-8708-89a5f8a25609",
   "metadata": {},
   "source": [
    "In How many Spark Partitions, below files will be read\n",
    "\n",
    "54 parquet files, 63 MB each, all 3 config parameters at default, No. of core equal to 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9daf23-7575-42a9-87d5-1d4c01b5c4ed",
   "metadata": {},
   "source": [
    "54 parquet files, 40 MB each, all 3 config parameters at default, No. of core equal to 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733b198-8af1-4f2b-a469-5365bdd039b3",
   "metadata": {},
   "source": [
    "54 parquet files, 40 MB each, maxPartitionBytes set to 88 MB, other two configs at default values., No. of core equal to 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
